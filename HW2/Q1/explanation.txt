1. On increasing the minimum support threshold, the time taken by all the algorithms decreases. This is because the total number of frequent subgraphs decreases. Since these algorithms take advantage of the Apriori property, they do not generate more candidates using infrequent subraphs, and as a result terminate quicker. On decreasing the support threshold, the growth rate (increase in time) increases. This is because the number of frequent subgraphs grows exponentially and hence, the algorithms take exponentially more time to finish. And when thresholds are very high, growth rate becomes constant.
2. FSG and gSpan take similar amount of time in most of the support thresholds. This might be because these algorithms differ mostly on the candidate generation step and not on the subgraph isomorphism part. However, gSpan is bit fast than FSG in support 10-50, the reason being that gSpan does not have to do candidate generation and false candidate test. At 0 support, the candidtes are actually very high, so the benifit of false candidate generation is not helping and hence the time taken by FSG is less.
3. Gaston performs better than both FSG and gSpan. From the Gaston paper, it seems like gaston is using a special data structure called Embedding List for each candidate subgraph to make the graph frequency counting step much faster than in the other algorithms. FSG and gSpan may keep a an array of identifier of graphs in the dataset for which a given candidate is a subgraph of. Embedding lists also include the identifier of the node involved in the last extension for each graph in the array. Using this structure they might have saved time on subraph isomorphism. And when the supports are low, these subgraph isomorphism tests will be high(because of more frequent graphs), so gaston performs extremely well there.
 
